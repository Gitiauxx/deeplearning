<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Deep Learning</title>
    <description></description>
    <link>http://localhost:4000/deeplearning/</link>
    <atom:link href="http://localhost:4000/deeplearning/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 30 Nov 2019 15:17:01 -0500</pubDate>
    <lastBuildDate>Sat, 30 Nov 2019 15:17:01 -0500</lastBuildDate>
    <generator>Jekyll v4.0.0</generator>
    
      <item>
        <title>Notes on Adversarial Learning</title>
        <description>&lt;p&gt;This post collects notes on a neural network vulnerability known as adversarial
examples. Adversarial examples are inputs designed by 
perturbing an actual input so that the neural network
outputs a wrong answer with high confidence, although the perturbed 
input is not perceptually different from the original input.&lt;/p&gt;

&lt;h2 id=&quot;what-are-adversarial-examples&quot;&gt;What are adversarial examples?&lt;/h2&gt;
&lt;p&gt;Adversarial examples are inputs to a neural network that are maliciously crafted so that
the neural network misclassifies them with high confidence. A typical example is a neural network that categorizes images b
by animal species. The left input is a a picture that is correctly classified as a panda; the right
input is the original image perturbed by the noise shown in the middle. Although there is no 
perceptual differences between the left and right images – they look like pandas – the neural network
is very confident that the right input corresponds to the picture of a gibbon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../assets/img/pandas.png&quot; alt=&quot;Adversarial Example&quot; /&gt;&lt;em&gt;Adversarial Example (source: &lt;a href=&quot;https://arxiv.org/pdf/1412.6572.pdf&quot;&gt;Goodfellow et al.&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-does-it-matter&quot;&gt;Why does it matter?&lt;/h2&gt;
&lt;p&gt;The lack of robustness to small perturbations is seen as a sign that 
neural networks do not learn the true data manifold. Moreover, adversarial
attacker could use that weakness to generate visually undetectable 
input perturbations that lead to a misclassified output. Such attacks 
could have dramatic consequences when applied to system with safety or security 
concerns. Think of how problematic it would be if an attacker can paint stop signs so that 
deep learning algorithms deployed in autonomous vehicles would recognize them as
a yield sign.&lt;/p&gt;

&lt;h2 id=&quot;what-causes-this-vulnerability&quot;&gt;What causes this vulnerability?&lt;/h2&gt;
&lt;p&gt;Initially, non-linearities were thought to explain the lack of robustness to adversarial 
inputs. However, &lt;a href=&quot;https://arxiv.org/pdf/1412.6572.pdf&quot;&gt;Goodfellow et al.&lt;/a&gt; show that 
linear perturbations of non-linear models are 
sufficient to generate misclassification. Consider an image $\textbf{x}$ represented by integer
pixel value between 0 and 255. Any perturbation within 1/255 precision should not 
lead to different model output. Consider a perturbation $\eta$ within 1/255 precision and the
resuling perturbed input $\overline{x}$.
The linear combination of the perturbed image with a weight vector $\mathbf{w}$ and $\overline{x}$, 
&lt;script type=&quot;math/tex&quot;&gt;w^{T}\overline{x} = w^{T}x + w^{T}\eta&lt;/script&gt; is optimized by choosing $\eta=\epsilon sign(w)$. 
Therefore, a $\epsilon$ perturbation on each pixel of the image $\mathbf{x}$ leads to
a total perturbation of magnitude $\epsilon m n$ after the linear combination, where 
$m$ is the average value of the weight $\mathbf{w}$ across all dimensions and $n$ is the dimension
of $\mathbf{w}$. For high dimensional problems, the resulting perturbation can be large enough to
affect the final prediction of the model.&lt;/p&gt;

&lt;h2 id=&quot;how-to-generate-adversarial-examples&quot;&gt;How to generate adversarial examples?&lt;/h2&gt;
&lt;p&gt;We have to distinguish between:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;White-box&lt;/em&gt; models where the architecture and parameters values are available to the 
 adversary&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Black-box&lt;/em&gt; models where the adversary does not access to the parameter values of the model, but can query
 the model for inputs of his choice.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-gradient-method&quot;&gt;Fast Gradient Method&lt;/h3&gt;
&lt;p&gt;The Fast Gradient Method (FGM) is a simple optimization procedure that has been shown to 
 have a high success rate at generating adversarial examples (see &lt;a href=&quot;https://arxiv.org/pdf/1412.6572.pdf&quot;&gt;Goodfellow et al.&lt;/a&gt;). 
 Consider a white-box model with known parameters $\mathbf{\theta}$ and 
 loss function $\mathcal{L}(x, y, \mathbf{\theta})$. For a given clean input $x$
 and associated label $y$, the perturbation $\eta$ that maximizes 
 $\mathcal{L}(x + \eta, y, \mathbf{\theta})$ is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta = \epsilon sign(\nabla \mathcal{L}_{x}(x + \eta, y, \mathbf{\theta})),&lt;/script&gt;

&lt;p&gt;where the gradients can be computed efficiently by backpropagation. The method is exact
for logistic classification and has high success rate for larger-non linear models.&lt;/p&gt;

&lt;h3 id=&quot;iterative-extension-of-fast-gradient-method&quot;&gt;Iterative extension of Fast Gradient Method&lt;/h3&gt;
&lt;p&gt;A natural extension og FGM is to proceed iteratively and apply the fast-gradient method to adversarial
example generated at the previous iteration:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}_{adv, 0} = \mathbf{x}; \: \mathbf{x}_{adv, k} = Clip_{\epsilon}(\mathbf{x}_{adv, k-1} + \alpha sign(\nabla \mathcal{L}_{x}(\mathbf{x}_{adv, k-1},  y, \mathbf{\theta})))&lt;/script&gt;

&lt;h3 id=&quot;robustness-to-transformation-by-cameras&quot;&gt;Robustness to transformation by cameras&lt;/h3&gt;
&lt;p&gt;FGM is a simple and cheap method that has been shown to be very effective at generating adversary example.
It can be expanded to more realistic cases where generated examples will be seen by the classifier
only through a camera (that can potentially remove the perturbations introduced in the
adversarial example).  &lt;a href=&quot;https://arxiv.org/pdf/1607.02533.pdf&quot;&gt;Kurakin et al.&lt;/a&gt; show that neural networks are still vulnerable to perturbed inputs that
are seen by the neural network via a third party medium (e.g. camera, sensors).
They generate adversary examples and pass them to the classifier through a cell phone camera.
Therefore, adversary replacing in the real world stop signs by their perturbed counterpart could force
an autonomous vehicle to misclassify those signs as yield signs.&lt;/p&gt;

&lt;h3 id=&quot;transferability-and-attack-on-black-box-models&quot;&gt;Transferability and Attack on Black-Box Models&lt;/h3&gt;
&lt;p&gt;White-box attacks are not very realistic, since vulnerabilities can be avoided by preventing
attackers to access the model’s parameters and architectures. However, 
it has been shown empirically that adversarial examples generalize to models
they were not trained against. That is, examples can be learned adversarially
against a child model and then use against its parent model if it is trained with a similar
dataset as the child model.&lt;/p&gt;

&lt;p&gt;In practice, the attacker queries model A (parent) for n inputs $x_{i}$ and obtained model’s A
predictions $y$. The attacker trains a model B (child) to predict the queried $y_{i}$ from inputs 
$x_{i}$. Since the attacker knows the structure of model B, he can launch a white-box attack on
model B and obtained adversarial inputs $x_{i, adv}$ for any $i=1, …, n$. &lt;strong&gt;Papers&lt;/strong&gt; show that 
these adversarial examples can be used against the original model A with high success rate.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-neural-network-what-does-it-take-to-make-robust-to-adversarial-examples&quot;&gt;Bayesian Neural Network: what does it take to make robust to adversarial examples?&lt;/h2&gt;
&lt;p&gt;The literature has developed several defenses against adversarial examples, including defense distillation
and adversarial learning –generate adversarial examples during training. In &lt;strong&gt;Paper&lt;/strong&gt;, they explore sufficient conditions 
for neural network to be robust to adversarial example.&lt;/p&gt;

&lt;p&gt;Consider idealized Bayesian Neural Network that are continuous models with high confidence on the training set.
It means that around each sample in the training set , there exists a small neighborhood around which 
the BNN has high confidence. Outside of those neighborhoods, if the BNN has low confidence,
then there exists no adversarial example. It seems that the key takeaway from this result is that 
robustness to adversarial examples relates to how fast uncertainty increases as a BNN sees more 
misclassified examples and how slowly output probability decreases from training samples.&lt;/p&gt;

</description>
        <pubDate>Tue, 26 Nov 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/deeplearning/2019/11/26/adversarial.html</link>
        <guid isPermaLink="true">http://localhost:4000/deeplearning/2019/11/26/adversarial.html</guid>
        
        
      </item>
    
      <item>
        <title>Sample Post with All Elements</title>
        <description>&lt;p&gt;This post shows all customized elements.
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo.&lt;/p&gt;

&lt;h2 id=&quot;image&quot;&gt;Image&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://via.placeholder.com/768x480&quot; alt=&quot;Placeholder&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;header&quot;&gt;Header&lt;/h2&gt;

&lt;h1 id=&quot;head-1&quot;&gt;Head 1&lt;/h1&gt;
&lt;h2 id=&quot;head-2&quot;&gt;Head 2&lt;/h2&gt;
&lt;h3 id=&quot;head-3&quot;&gt;Head 3&lt;/h3&gt;
&lt;h4 id=&quot;head-4&quot;&gt;Head 4&lt;/h4&gt;
&lt;h5 id=&quot;head-5&quot;&gt;Head 5&lt;/h5&gt;
&lt;h6 id=&quot;head-6&quot;&gt;Head 6&lt;/h6&gt;

&lt;h2 id=&quot;lists&quot;&gt;Lists&lt;/h2&gt;

&lt;p&gt;Unordered list&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I am the first unordered list item&lt;/li&gt;
  &lt;li&gt;I am the second unordered list item&lt;/li&gt;
  &lt;li&gt;I am the third unordered list item&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ordered list&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;I am the first ordered list item&lt;/li&gt;
  &lt;li&gt;I am the second ordered list item&lt;/li&gt;
  &lt;li&gt;I contain an &lt;code class=&quot;highlighter-rouge&quot;&gt;inline code&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;code-block&quot;&gt;Code block&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'hello, world'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'this is a really long statements, this is a really long statementsi, this is a really long statements'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;inline-code&quot;&gt;Inline code&lt;/h2&gt;

&lt;p&gt;Ut enim ad minima veniam, &lt;code class=&quot;highlighter-rouge&quot;&gt;quis&lt;/code&gt; nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, &lt;code class=&quot;highlighter-rouge&quot;&gt;vel&lt;/code&gt; illum qui dolorem eum &lt;code class=&quot;highlighter-rouge&quot;&gt;fugiat&lt;/code&gt; quo voluptas nulla pariatur?&lt;/p&gt;

&lt;h2 id=&quot;blockquote&quot;&gt;Blockquote&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;paragraph&quot;&gt;Paragraph&lt;/h2&gt;

&lt;p&gt;Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc, blandit vel, luctus pulvinar, hendrerit id, lorem. Maecenas nec odio et ante tincidunt tempus. Donec vitae sapien ut libero venenatis faucibus. Nullam quis ante. Etiam sit amet orci eget eros faucibus tincidunt. Duis leo.&lt;/p&gt;
</description>
        <pubDate>Sat, 26 Sep 2015 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/deeplearning/2015/09/26/sample-post-with-all-elements.html</link>
        <guid isPermaLink="true">http://localhost:4000/deeplearning/2015/09/26/sample-post-with-all-elements.html</guid>
        
        
      </item>
    
      <item>
        <title>How to Use Book</title>
        <description>&lt;p&gt;Simple is better, but many themes are over designed.
So, &lt;a href=&quot;https://github.com/kkninjae/book&quot;&gt;Book&lt;/a&gt; is born.&lt;/p&gt;

&lt;h2 id=&quot;get-started&quot;&gt;Get started&lt;/h2&gt;

&lt;p&gt;You should be able to use Book without significant change.
But make sure you have modified &lt;code class=&quot;highlighter-rouge&quot;&gt;baseurl&lt;/code&gt; properly before using it.&lt;/p&gt;

&lt;p&gt;After that, you can follow the steps below to run it locally.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Download this repository
    &lt;ul&gt;
      &lt;li&gt;Use Git to clone it, or&lt;/li&gt;
      &lt;li&gt;Download it via &lt;a href=&quot;https://github.com/kkninjae/book/archive/master.zip&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Read &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; and change the default settings as your need&lt;/li&gt;
  &lt;li&gt;Install &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll&lt;/code&gt; and run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve -w&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;tips&quot;&gt;Tips&lt;/h2&gt;

&lt;p&gt;Use absolute path when you change values of &lt;code class=&quot;highlighter-rouge&quot;&gt;favicon&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;avatar&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When you write posts, there is no need to specify a layout for each post.
&lt;code class=&quot;highlighter-rouge&quot;&gt;post&lt;/code&gt; layout is the default value for any posts.
Otherwise, specify a different layout value if you do not want to use &lt;code class=&quot;highlighter-rouge&quot;&gt;post&lt;/code&gt; layout.&lt;/p&gt;

&lt;p&gt;If you are using &lt;a href=&quot;https://disqus.com/&quot;&gt;Disqus&lt;/a&gt; commenting system, commenting is turned on for every post by default. You can set &lt;code class=&quot;highlighter-rouge&quot;&gt;disqus: false&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; file to disable comment for all posts. Or, you can add &lt;code class=&quot;highlighter-rouge&quot;&gt;comment: false&lt;/code&gt; in the front matter of the page you want to disable comment. As you can see, this post disables comment.&lt;/p&gt;

&lt;h2 id=&quot;end&quot;&gt;End.&lt;/h2&gt;

&lt;p&gt;If you like it, why not give it a &lt;a href=&quot;https://github.com/kkninjae/book&quot;&gt;star&lt;/a&gt; :).&lt;/p&gt;
</description>
        <pubDate>Fri, 28 Aug 2015 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/deeplearning/2015/08/28/how-to-use-book.html</link>
        <guid isPermaLink="true">http://localhost:4000/deeplearning/2015/08/28/how-to-use-book.html</guid>
        
        
      </item>
    
  </channel>
</rss>
